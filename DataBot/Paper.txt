Optimization tools are needed in every step of an accelerator project,from the design to commissioning to operations. However,different phases have different optimization needs that may require different optimization algorithms. For example,a global optimizer is more appropriate in the design phase to map the whole parameter space whereas a local optimizer with a shorter path to solution is more adequate during operations to find the next best operating point. Different optimization algorithms are being used in accelerator physics,we mention in particular standard algorithms such as least square minimization and evolutionary algorithms such as genetic optimization. Over the years,we have developed several optimization tools for beam tracking codes to include 3D fields and SC effects. Including particle tracking in the optimization process calls for parallel computing. We will review the different algorithms and their implementation and present few highlight applications.

Optimizations are heavily used in the design phase of an accelerator project, but they are much less used to support the commissioning and operations once the machine is built. During the design phase, optimizations are used in the design of the different beam line elements: magnets, rf cavities, etc. They are also used for the lattice optimization to find the appropriate sequence of elements and drift spaces. Once the lattice is defined more optimizations are used to determine the appropriate element settings for optimal beam dynamics and beam quality. This is often iterated with the lattice design. Once the accelerator is built, more effort is dedicated to hardware problems than to developing a realistic model of the machine. We believe that using the appropriate optimization tools during the commissioning should help better understand the machine’s behaviour and expedite the delivery of the first beam. Fits to reproduce the experimental data using a model should significantly improve the predictability of the model to use for real- time machine operations. Often, simplified models (1D, single particle) are used to support daily machine operations. Simple models have the advantage of being fast and able to describe the overall behaviour of the machine while detailed 3D models are slow and still cannot reproduce the details seen in the data. We believe that a significant effort should be dedicated to developing more realistic 3D models before being able to use them to support real-time machine operations. Once such models are fully developed large scale parallel computing could be used for fast turn-around simulations and optimizations.

The first important step is the proper definition of the optimization problem. An optimization problem has one or more objectives which are the important quantities or qualities characteristics of the problem that you would like to optimize. These objectives depend on the parameters of the problem which are the variables affecting the outcome or the solution to the problem. It is usually a good practice to choose the parameters to which the solution is more sensitive. These parameters could be subject to constraints and or correlations which define the limits of the parameter space. The simplest case is where the parameters are independent with lower and upper bounds. If the parameters are correlated, it is usually recommended to reduce them into a set of independent parameters. The last and most important element of an optimization problem is the choice of the appropriate optimization algorithm. Depending on the nature of the problem, the most appropriate algorithm could be a local optimizer, a global one, a standard or an evolutionary.

It is usually not hard to find a local minimum of an objective function. What is hard is to prove that the minimum found is a good one and it is even harder to prove that a minimum is an absolute or a global one. A local optimizer usually starts with a first guess then finds a direction that minimizes the objective function and moves one step in that direction. The procedure is repeated iteratively until no more progress could be made. A local algorithm is usually fast because it explores the parameter space along a single path defined by the minimization direction adjusted at every step. In contrast, a global optimizer should explore the entire parameter space and eventually find all local minima before finding a global one. It should also prove that the minimum found is a global one which makes it much slower than a local optimizer. Luckily, not all problems or applications require global optimizations. A global optimizer is more appropriate to use in the design phase of an accelerator project to map the whole parameter space and make sure not to miss the best set of design parameters. Such a global optimizer should also find all feasible solutions to help make compromises if needed. On the other hand, a local optimizer with a shorter path to solution is more adequate to support accelerator operations. In this case, we usually start from a good starting point to find the next best operating point by returning few elements on the accelerator. The time to solution is a very important parameter in the choice of the appropriate optimization algorithm. A good optimizer should also optimize the path to the best solution.

Standard optimization algorithms are the most common and widely used. They usually have a single objective function to optimize which could be a weighted sum of multiple objectives. Derivatives of the objective function with respect to the optimization parameters are usually required. A single trial solution is evaluated at every iteration. Evolutionary algorithms are more recent and are based on the “Theory of Evolution and Natural Selection”, where only the best survive. Multiple objective functions could be included in the optimization without the requirement of knowing their derivatives. Multiple trial solutions are evaluated at every iteration which gives the global nature of evolutionary algorithms.

The first example of standard optimization algorithms is the simplex method, which does not require the function derivatives. It starts by building a simplex consisting of the first guess and a base of feasible solutions in the parameter space. Iteratively, the worst solution is replaced by a better one built from the base. The iterations stop when no more progress could be made or at a predefined cut-off error on the solution. The second example is the gradient descent method which require the first derivatives of the objective function on the optimization parameters. The derivatives are used to determine the minimization direction pi at every iteration where f is the objective function and B is a symmetric non singular matrix. In the case where B is the matrix identity I the method is called the steepest descent method. The third example is the Newton method which requires both the first and second derivatives of the objective function. In this case B is the matrix of second derivatives also known as the Hessian matrix H. The fourth example is the Quasi-Newton method which uses the first derivatives but keeps updating the matrix of second derivatives at every iteration. In this case B is an approximation to the Hessian matrix and does not require extra computing effort for the second derivatives as for the Newton method. Each of these methods is actually a class of methods with a variety of implementations.

A genetic optimizer starts with a set of solutions randomly generated inside the parameter space. The solutions are evaluated and ranked based on the objectives and constraints of the problem to select a subset of best solutions. The selected solutions are then used to generate the next population of solutions by crossover, mutation or other using predefined rates. The evaluation and ranking procedure is repeated for the new set of solutions until no progress could be made or a stable set of best solutions is obtained. For a given solution, the array of parameter values plays the role of a gene. Figure 1 shows the different ways of generating the next generation of solutions from the selected set of best solutions, namely crossover, mutation and random. The random generation of new solution could be turned on at the beginning for better sampling of the parameter space then turned off.

Standard algorithms are serial in nature because the direction of the next iteration is decided based on the outcome of the current one. The fact that a single solution is evaluated per iteration makes these algorithms parallelizable only at the level of the objective function and derivatives evaluation. For example, a least square minimization where the objective function is of the form:

could be parallelized by parallelizing the sum for large N. Optimizations using multi-particle tracking could be parallelized by parallelizing the tracking, the Poisson solver and the statistics calculations for large number of particles. A global optimizer may be parallelized by subdividing the parameter space and assigning the different sub-spaces to different processors.

In contrast Evolutionary algorithms are parallel in nature because at every iteration multiple solutions are evaluated independently which makes them well suited for parallel processing with minimal communication. It is however not easy to parallelize the ranking, selection and offspring generation which are usually assigned to the master process. This makes evolutionary algorithms more suitable for optimization using multi-particle codes with realistic 3D external and space charge fields. Usually no parallel particle tracking is required unless a very large number of particles is needed for the optimization problem. In practice, any tracking code with space charge calculations could be used. A higher level parallel layer is often used to manage the generation, ranking and selection of trial solutions and every process calls the code when needed. In our beam dynamics code TRACK this layer was built-in within the code.

In this application, a longitudinal fine-tuning procedure was developed specifically for a multiple charge state beam to minimize its longitudinal emittance right before a stripper. The beam should reach the stripper in the form of an up-right ellipse in the (Δφ, ΔW) plane to minimize the emittance growth from the energy straggling effect in the stripper. This could be realized by matching the beam centroids and Twiss parameters of the different charge state beams. The objective function in this case is:

where W0 is the desired beam energy and εW is the associated error.εΔW ,εΔφ and εα are the allowed errors on the relative energy, phase and α shifts of the individual charge state beams from the central beam. The fit parameters in this case are the RF cavities phases and amplitudes in the section up-stream of the stripper. Figure 2 shows the results of the fit for a five charge state uranium beam in the medium energy section of the RIA driver linac. This optimization reduced beam losses in the high-energy section of the linac by a significant factor as seen on figure 3.

We have recently developed a realistic corrective steering procedure. A simplified algorithm is presented in figure 4.In TRACK implementation, instead of solving the matrix equation A*C+B=0 for the array of corrector field strengths C, with A being the response function of monitors to correctors and B the monitors readings for C=0, we perform a least square minimization of the equivalent function given below:

In this way, we can include the monitor precision σ im and the maximum corrector field strength C max in the solution. Monitors with different precisions will have different weights in the minimization procedure. The minimization should lead to an approximate solution in the case of an over-determined problem (more equations than variables) and to the best solution in the case of multiple solutions (under-determined problem). This realistic corrective steering procedure could very well be applied in the design phase of an accelerator project to determine the monitors and correctors requirements for an effective beam center correction as well as in the control room of an operating accelerator. The results of its application during the design of the HINS project front-end linac being built at Fermilab are shown on figure 5. After multiple iterations, the optimum numbers and locations of correctors and monitors for an effective correction were determined.

We have recently implemented a parallel genetic optimizer into the beam dynamics code TRACK. As a first application we used it for the design optimization of the first chicane of an electron injector for an X-FEL-O linac. The objective was to minimize the transverse emittance at the end of the chicane. The parameters were the quadrupoles and solenoids strengths as shown in figure 6. We were able to reduce the transverse emittance by about 10% from a manually optimized case which is very critical for an ultra-low emittance injector.

To study the convergence of the genetic optimizer we compared the corresponding results to the results from a 2D map in the case of two parameter optimization. Figure 7 shows the results for both cases proving the convergence of the genetic optimizer in 5 iterations corresponding to a total of 500 trial solutions. Large scale optimizations are underway for a larger section of the linac.

The concept of the model driven accelerator is to develop and use a computer model to support real-time accelerator operations. Presently, no accelerator in the world could fully rely on a computer model for its operations. The main reason is a discontinuity between the design and operation phases of an accelerator project. Among the factors contributing to this discontinuity are: 1-Simulations in the design phase assume almost perfect conditions and cannot reproduce the real machine, 2-Actual elements specification and performance are usually different from their original design and in most cases 3-Not enough diagnostic devices to characterize the machine. The lack of a realistic model to support the commissioning and operations results in significant delay in the deployment of a new machine and a lot of time spent on machine tuning during operations. This usually leads to low availability and high operating cost of the machine. For example, a complex system such as the proposed FRIB facility, where primary beams from proton to uranium up to 600 MeV/u are used to produce beams of rare isotopes all over the map, cannot afford not to have a computer model to support its operations.

To bridge the gap between the design and operation phases we propose to develop a realistic model of the machine. Among the benefits of such a model is fast tuning for the desired beam conditions and fast retuning to restore the beam after a failure. This should significantly improve the availability of the machine and reduce its operating cost.

The main requirements for the realization of the model driven accelerator could be summarized in the development of a 3D beam dynamics code with the appropriate set of optimization tools and large scale computing capabilities. A multi-particle beam dynamics code is more realistic than matrix-based and single- particle codes because it supports 3D fields, includes fringe fields and appropriate space charge calculations. It also allows more detailed simulations necessary to study eventual beam loss and produce data similar to the measured data. Such a code should also include a large set of optimization tools. Optimization tools are needed not only for design optimization but also to tailor the computer model to the actual machine to be useful for real-time operations. Multi-particle optimizations usually involve tracking a large number of particles for large number of iterations which is very time consuming and requires large scale parallel computing. Therefore the beam dynamics code should have parallel computing capabilities.

We have recently succeeded to extract, accelerate, analyze and recombine a two-charge state DC bismuth beam from an ECR ion source. The beam is perfectly combined at the point of injection into a subsequent RF accelerator. We consider this as a small scale realization of the concept of the model driven accelerator. The beam dynamics code TRACK was used first to design the beam line and then to support the operation by predicting the elements settings required to recombine the two charge state beam at the end of the LEBT. Figure 8 is a general 3D view of the prototype 2Q-LEBT beam line.

For a realistic beam dynamics simulation, 17 beams (O and Bi) are tracked simultaneously from the ion source through the LEBT with a total current at the source of about 2 mA. We assumed a 4D water-bag initial distribution for all beams and a 50 % charge compensation factor in non-electric devices and 0% in electric devices. Realistic 3D models were developed and used for all beam line elements. In order to tailor the TRACK model to the actual beam line we had first to determine the initial beam parameters at the source. To do so we had to develop a new procedure to fit the beam profiles measured at the middle plane by varying the beam parameters at the source. Figure 9 shows the result of the fit for a two-charge state 75-kV bismuth beam (20+, 21+). The transverse emittances and Twiss parameters obtained by fit show that despite the axial symmetry of the extraction region, the beam is not axial symmetric which could be explained by a non symmetric plasma boundary inside the ion source.


Once the initial beam conditions are known, we used the computer model to find the element settings for the desired operation mode. A new fit procedure was developed to produce symmetric beam dynamics between the two bending magnets as this is a necessary condition to recombine the multiple charge state beams. Another fit was also used to find the setting of the last triplet for a perfect combination at the end of the LEBT where a beam profile monitor and a Pepper-Pot emittance meter are installed. Figure 10 shows the measured beam profiles and figure 11 shows the Pepper-Pot images at the end of the LEBT. A comparison of the element settings predicted by TRACK and the actual setting to combine the two beam shows maximum deviation of ~ 10 % which could be improved by checking the assumptions made in the simulations. We notice that the two charge state beams are almost perfectly combined.

To be able to use a realistic 3D model online for real-time machine operations, we should be able to perform large scale optimizations on large number of processors (32768 processors or more). More optimization tools need to be developed for the commissioning phase to tailor the computer model to the actual machine by fitting the measured data. For this purpose, interfaces between the beam diagnostic devices and the computer model are needed to calibrate and analyze the data to input to the code. Numerical experiments could be used to test and fine tune the tools before implementation to the real machine by producing detector-like data. Only after all these developments, that a realization of the model driven accelerator will be possible. As a full scale application, we are proposing to apply this concept to the superconducting linac ATLAS at Argonne and to other existing machines.

Optimization tools and methods are needed in every phase of an accelerator project, namely the design, commissioning and operations. No single algorithm could satisfy all these optimization needs. Different algorithms are being used in accelerator physics: local, global, standard and evolutionary. We have briefly reviewed and compared different classes of optimization algorithms and presented few applications in beam dynamics optimization. The ultimate goal of realizing the concept of the “Model Driven Accelerator” will require the development of a realistic 3D beam dynamics code with the appropriate set of optimization tools and large scale parallel computing capabilities. For new machines, we should take advantage of the commissioning phase to bridge the gap between the original design and the actual machine by tailoring the computer model to the machine. Obviously, a significant development effort is still needed for a full scale realization of the concept of the “Model Driven Accelerator”.